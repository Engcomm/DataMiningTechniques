{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from a_prep_data import prep_data\n",
    "from b_tokstemlemm_data import token_stem_lemm\n",
    "from c_analysis import analysis\n",
    "from d_bow import bow\n",
    "from e_tfidf import tfidf\n",
    "from f_emb import emb\n",
    "from g_extrafeatures import extrafeat\n",
    "from h_merge_embextra import merge_embextra\n",
    "from i_svm_class import svm\n",
    "from j_knn_class import knn\n",
    "from k_rr_class import rr\n",
    "from k_rr_probs_class import rr_probs\n",
    "from l_results import results\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from aux import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform auxiliary actions\n",
    "## Create the files' output directory if it doesn't exist\n",
    "## Create a file with the classification (numeric representation {-1: negative, 0: neutral, 1: positive}) \n",
    "## of the training data\n",
    "## Create files with the indices of the training data per class\n",
    "## Create a file with the correct classification (numeric representation {-1: negative, 0: neutral, 1: positive}) \n",
    "## of the training data\n",
    "## Create a file with the hashtags only, removing the duplicates per tweet\n",
    "aux()\n",
    "\n",
    "\n",
    "# A: Pre-processing of the training & testing data\n",
    "## lowercase\n",
    "## rm unicode characters\n",
    "## rm mentions, hashtags, links\n",
    "## rm emoticons\n",
    "## rm numbers, punctuation, symbols\n",
    "## rm stop-words,\n",
    "## extra stop-words: days (also their short forms), months (also their short forms), rt\n",
    "## rm multiple consecutive spaces, stripping\n",
    "prep_data()\n",
    "\n",
    "\n",
    "# B: Tokenization, stemming and lemmatization of the training & testing data\n",
    "## nltk Porter stemmer\n",
    "## nltk WordNetLemmatizer\n",
    "token_stem_lemm()\n",
    "\n",
    "\n",
    "# D: Bag-of-words\n",
    "## sklearn CountVectorizer\n",
    "## Parameters: max_features=50, max_df=0.5, min_df=1\n",
    "bow()\n",
    "\n",
    "\n",
    "# E: TF-IDF\n",
    "## sklearn TfidfVectorizer\n",
    "## Parameters: max_features=50, max_df=0.5, min_df=1\n",
    "tfidf()   \n",
    "\n",
    "\n",
    "# F: Word embedding\n",
    "## gensim Word2Vec\n",
    "## Parameters: size=50 (200-300 made the process very slow), window=10, min_count=2, sg=0 & 1, epochs=10\n",
    "## The random mean vectors for tweets with no words belonging to the vocabulary\n",
    "## were generated with sampling the vocabulary 100 times with replacement\n",
    "## * 'GoogleNews-vectors-negative300.bin' and 'freebase-vectors-skipgram1000-en.bin' \n",
    "##    resulted in memory error so they were not used\n",
    "emb(w2v_sg=0)\n",
    "emb(w2v_sg=1)\n",
    "\n",
    "\n",
    "# G: Extra features\n",
    "## Features: mean / min. / max. valence per tweet per dictionary, length of the tweet, \n",
    "## no. words / emoticons / exclamation marks / mentions / hashtags / URLs / vowels of the tweet,\n",
    "## count of hashtags among the top 20 per class\n",
    "## Dictionaries: affin, generic\n",
    "## (the rest of the dictionaries took much time to load and the process was slow so they were omitted)\n",
    "## The random vectors for tweets with no words belonging to the dictionary\n",
    "## were generated with sampling the vocabulary 100 times with replacement\n",
    "extrafeat()\n",
    "\n",
    "\n",
    "# H: Merge the word embedding results with the extra features\n",
    "merge_embextra()\n",
    "\n",
    "\n",
    "# C: Analysis of the training data\n",
    "## Pie-chart: classes\n",
    "## Wordcloud (per class, total): all words (pre-processed data), top 20 hashtags\n",
    "## Histogram (per class, total, combined): per extra feature\n",
    "## No. tweets (per class, total): top 15 hashtags\n",
    "analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- C: Analysis images -->\n",
    "\n",
    "## Pie-chart\n",
    "<table>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/pie_class.png\" /></td></tr>\n",
    "</table>\n",
    "\n",
    "## Wordclouds: words\n",
    "<table>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/wordcloud_lemm_total.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/wordcloud_lemm_pos.png\" /></td></tr>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/wordcloud_lemm_neu.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/wordcloud_lemm_neg.png\" /></td></tr>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/wordcloud_stemm_total.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/wordcloud_stemm_pos.png\" /></td></tr>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/wordcloud_stemm_neu.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/wordcloud_stemm_neg.png\" /></td></tr>\n",
    "</table>\n",
    "\n",
    "## Wordclouds: hashtags \n",
    "<table>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/wordcloud_hashtags_total.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/wordcloud_hashtags_pos.png\" /></td></tr>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/wordcloud_hashtags_neu.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/wordcloud_hashtags_neg.png\" /></td></tr>\n",
    "</table>\n",
    "\n",
    "## No. words \n",
    "<table>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_words_total.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_words_pos.png\" /></td></tr>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_words_neu.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_words_neg.png\" /></td></tr>\n",
    "</table>\n",
    "\n",
    "## No. vowels \n",
    "<table>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_vowels_total.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_vowels_pos.png\" /></td></tr>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_vowels_neu.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_vowels_neg.png\" /></td></tr>\n",
    "</table>\n",
    "\n",
    "## No. characters \n",
    "<table>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_len_total.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_len_pos.png\" /></td></tr>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_len_neu.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_len_neg.png\" /></td></tr>\n",
    "</table>\n",
    "\n",
    "## No. emoticons \n",
    "<table>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_emoticons_total.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_emoticons_pos.png\" /></td></tr>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_emoticons_neu.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_emoticons_neg.png\" /></td></tr>\n",
    "</table>\n",
    "\n",
    "## No. hashtags \n",
    "<table>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_hashtags_total.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_hashtags_pos.png\" /></td></tr>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_hashtags_neu.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_hashtags_neg.png\" /></td></tr>\n",
    "</table>\n",
    "\n",
    "## No. URLs \n",
    "<table>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_urls_total.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_urls_pos.png\" /></td></tr>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_urls_neu.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_urls_neg.png\" /></td></tr>\n",
    "</table>\n",
    "\n",
    "## No. mentions\n",
    "<table>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_mentions_total.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_mentions_pos.png\" /></td></tr>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_mentions_neu.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_mentions_neg.png\" /></td></tr>\n",
    "</table>\n",
    "\n",
    "## No. exclamation marks\n",
    "<table>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_excl_total.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_excl_pos.png\" /></td></tr>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_excl_neu.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_excl_neg.png\" /></td></tr>\n",
    "</table>\n",
    "\n",
    "## Mean, min., max. valence per dictionary\n",
    "### affin\n",
    "<table>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_mean_lex1_total.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_mean_lex1_pos.png\" /></td></tr>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_mean_lex1_neu.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_mean_lex1_neg.png\" /></td></tr>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_min_lex1_total.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_min_lex1_pos.png\" /></td></tr>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_min_lex1_neu.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_min_lex1_neg.png\" /></td></tr>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_max_lex1_total.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_max_lex1_pos.png\" /></td></tr>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_max_lex1_neu.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_max_lex1_neg.png\" /></td></tr>\n",
    "</table>\n",
    "\n",
    "### generic\n",
    "<table>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_mean_lex2_total.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_mean_lex2_pos.png\" /></td></tr>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_mean_lex2_neu.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_mean_lex2_neg.png\" /></td></tr>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_min_lex2_total.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_min_lex2_pos.png\" /></td></tr>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_min_lex2_neu.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_min_lex2_neg.png\" /></td></tr>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_max_lex2_total.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_max_lex2_pos.png\" /></td></tr>\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/hist_max_lex2_neu.png\" /></td>\n",
    "        <td><img style=\"float: left;\" src=\"gendata/img/hist_max_lex2_neg.png\" /></td></tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NN\n",
      "Round-robin\n"
     ]
    }
   ],
   "source": [
    "# I: SVM classification\n",
    "## linear, poly-2nd, poly-3rd, sigmoid, rbf\n",
    "svm(svm_kernel='linear')\n",
    "svm(svm_kernel='poly', svm_degree=2)\n",
    "svm(svm_kernel='poly', svm_degree=3)\n",
    "svm(svm_kernel='sigmoid')\n",
    "svm(svm_kernel='rbf')\n",
    "\n",
    "\n",
    "# J: kNN classification\n",
    "## (1, 3)\n",
    "knn(knn_n=1)\n",
    "knn(knn_n=3)\n",
    "\n",
    "\n",
    "# K: Round-robin classification\n",
    "## (1, 3) x (class, probs)\n",
    "rr(knn_n=1)\n",
    "rr(knn_n=3)\n",
    "\n",
    "rr_probs(knn_n=1)\n",
    "rr_probs(knn_n=3)\n",
    "\n",
    "\n",
    "# L: Results\n",
    "## classifiers x input types\n",
    "results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- L: Results images -->\n",
    "\n",
    "<table align=\"left\">\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/fscore.png\" /></td></tr>\n",
    "    <tr><td><p style=\"align:justify\"&nbsp;</p></td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "    <tr><td><img style=\"float: left;\" src=\"gendata/img/classratio.png\" /></td></tr>\n",
    "    <tr><td>\n",
    "        \n",
    "<div style=\"text-align:justify; align:justify\">\n",
    "\n",
    "<strong>SVM_linear:</strong> SVM classifier, linear kernel<br />\n",
    "<strong>SVM_poly_2nd:</strong> SVM classifier, 2nd degree polynomial kernel<br />\n",
    "<strong>SVM_poly_3rd:</strong> SVM classifier, 3rd degree polynomial kernel<br />\n",
    "<strong>SVM_rbf:</strong> SVM classifier, RBF kernel<br />\n",
    "<strong>SVM_sigmoid:</strong> SVM classifier, sigmoid kernel<br />\n",
    "<strong>kNN_1:</strong> 1 Nearest Neighbor classifier<br />\n",
    "<strong>kNN_3:</strong> 3 Nearest Neighbors classifier<br />\n",
    "<strong>RR_1_class:</strong> Round-robin classification using 1-NN and predict<br />\n",
    "<strong>RR_3_class:</strong></strong> Round-robin classification using 3-NN and predict<br />\n",
    "<strong>RR_1_probs:</strong> Round-robin classification using 1-NN and predict_proba<br />\n",
    "<strong>RR_3_probs:</strong> Round-robin classification using 3-NN and predict_proba<br />\n",
    "<br />\n",
    "\n",
    "<strong>BOW:</strong> Bag-of-words<br />\n",
    "<strong>TF-IDF:</strong> Term frequency, inverse document frequency<br />\n",
    "<strong>WE-CBOW:</strong> Word2Vec, CBOW training algorithm<br />\n",
    "<strong>WE-SG:</strong> Word2Vec, skip-gram training algorithm<br />\n",
    "<strong>Extra feat.:</strong> Extra features only<br /><br />\n",
    "\n",
    "\n",
    "<p style=\"text-align:justify; align:justify\">Παρατηρείται ότι καλύτερα αποτελέσματα προκύπτουν με τη χρήση του ταξινομητή k πλησιέστερων γειτόνων (kNN_1, kNN_3, RR_1_class, RR_3_class, RR_1_probs, RR_3_probs), με συγκρίσιμες - σχεδόν ίδιες τιμές F-score για όλες τις περιπτώσεις, με όλα δεδομένα εισσόδου εκτός των BOW και TF-IDF. Με τα δεδομένα BOW και TF-IDF, φαίνεται η τάση μείωσης του F-score. Η πολύ μικρή τιμή (0.08) στην περίπτωση που RR_1_class με TF-IDF, μπορεί να οφείλεται σε τυχαία γεγονότα, όπως η αρχικοποίηση των ταξινομητών 1-NN, και θα έπρεπε να επαναληφθεί η διαδικασία πολλές φορές ώστε η μέση τιμή του F-score να επιβεβαιώσει αν αυτό ισχύει.</p>\n",
    "\n",
    "<p style=\"text-align:justify; align:justify\">Επίσης, στο σύνολο των ταξινομητών, είναι συγκριτικά καλύτερα τα αποτελέσματα όταν τα δεδομένα εισόδου περιλαμβάνουν τα \"Extra feat.\". Πιθανώς η χρήση προ-εκπαιδευμένου μοντέλου των διανυσμάτων των λέξεων (\"GoogleNews-vectors-negative300.bin\", \"freebase-vectors-skipgram1000-en.bin\") να οδηγούσε σε καλύτερα αποτελέσματα. Περιορίζοντας τον αριθμό των λέξεων του μοντέλου \"GoogleNew-vectors-negative300\", στο 1000000, η τιμή του F-score, χρησιμοποιώντας τον ταξινομητή SVM, με πυρήνα RBF, ήταν 0.23 από 0.08, ενώ χρησιμοποιώντας τον ταξινομητή 3-NN, παρέμεινε αμετάβλητη. Ακόμη, μπορεί η διαδικασία να επηρεάζεται κι από την επιλογή παραγωγής τυχαίων μέσων τιμών, στη περίπτωση των διανσμάτων, μόνο όταν καμία λέξη του tweet δεν υπάρχει στο λεξικό του μοντέλου Word2Vec. Έτσι, κάποια tweets μπορεί να επηρεάζονται πολύ από μία μόνο λέξη.</p>\n",
    "\n",
    "<p style=\"text-align:justify; align:justify\">Επίσης, πριν την τελική διαμόρφωση των βημάτων, ελέγχθηκαν οι συνδυασμοί των παρακάτω:\n",
    "\n",
    "<ul><li>Χρήση των αποτελεσμάτων Ανάλυσης Κύριων Συνιστωσών - 3 διαστάσεις, 10 διαστάσεις (~20% εξηγούμενη διακύμανση των δεδομένων εκπαίδευσης), για μείωση της διαστατικότητας, αυξάνοντας τις διαστάσεις των δεδομένων στα οποία εφαρμόζεται, χωρίς να επιβραδύνεται πολύ η διαδικασία</li></ul>\n",
    "\n",
    "<ul><li>Διατήρηση των tweets με τη μορφή που παίρνουν μετά από τη λημματοποίηση ή την περιστολή, για την περίπτωση που μπορούσε να αυξηθεί η \"ομοιότητα\" αυτών με το ίδιο συναίσθημα και λέξεις με ίδια ρίζα και διαφορετική μορφή γραφής</li></ul>\n",
    "\n",
    "<ul><li>Μετασχηματισμός των δεδομένων με χρήση του RobustScaler, για να μη κυριαρχούν χαρακτηριστικά με υψηλότερες τιμές λόγω διαφορετικού εύρους τιμών (όπως ισχύει στα λεξικά) ή ακραίων παρατηρήσεων</li></ul>\n",
    "\n",
    "Τα αποτελέσματα δεν ήταν καλύτερα με κάποιον από τους παραπάνω συνδυασμούς.\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:justify; align:justify\">\n",
    "Επιπλεόν, βλεποντας τους λόγους ορθής ανά κλάση, στη δεύτερη εικόνα, φαίνεται πως οι ταξινομητές k-NN, έχουν συγκρίσιμες τιμές και στις 3 κλάσεις, σε αντίθεση με τους υπόλοιπους, που ταξινομούν πολύ καλύτερα 1 ή 2 κλάσεις και με εξαιρετικά χαμηλή επιτυχία τις υπόλοιπες 2 ή 1, ώστε να είναι μικρότερος ο λόγος κι από την περίπτωση τυχαίας ταξινόμησης.\n",
    "</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "        \n",
    "</td></tr>\n",
    "</table>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
